{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nadaramadan1/AI_Workshop_Projects/blob/main/Nada_Ramadan_Bone_Fructure_Detecion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBViOPP1s-5O",
        "outputId": "df5de6ea-0658-49ca-c23b-a93ec9313c54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/fracture-multi-region-x-ray-data\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"bmadushanirodrigo/fracture-multi-region-x-ray-data\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kI65RkFEtOS7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout,Flatten, Conv2D,BatchNormalization,MaxPooling2D,GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from IPython.display import SVG, Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import cv2\n",
        "import random\n",
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import shutil\n",
        "from shutil import copyfile\n",
        "import plotly.express as px\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs_JUkM0AP98"
      },
      "source": [
        "# **Explore Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hXmUQlntmeg"
      },
      "outputs": [],
      "source": [
        "train=path+'/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/train'\n",
        "test=path+'/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/test'\n",
        "val=path+'/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/val'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqfEAIwl6oOc"
      },
      "outputs": [],
      "source": [
        "train_dir = path + '/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/train'\n",
        "test_dir  = path + '/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/test'\n",
        "val_dir   = path + '/Bone_Fracture_Binary_Classification/Bone_Fracture_Binary_Classification/val'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(train_dir, target_size=(224, 224), batch_size=32, class_mode='binary')\n",
        "val_data = val_datagen.flow_from_directory(val_dir, target_size=(224, 224), batch_size=32, class_mode='binary')\n",
        "test_data = test_datagen.flow_from_directory(test_dir, target_size=(224, 224), batch_size=32, class_mode='binary')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiOQw1z01BZh",
        "outputId": "5f4dcfeb-cdc4-43be-a6d0-90e1f1f41bda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9246 images belonging to 2 classes.\n",
            "Found 829 images belonging to 2 classes.\n",
            "Found 506 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(224,224,3)),\n",
        "    MaxPooling2D(2,2),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Flatten(),\n",
        "    Dropout(0.5),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCFuVClF1Boo",
        "outputId": "dad3cd51-0336-4d4d-f776-02fbf9018cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n"
      ],
      "metadata": {
        "id": "UpSKg8qx1E5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "import os\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "\n",
        "def remove_corrupted_images(directory):\n",
        "    num_removed = 0\n",
        "    for subdir in os.listdir(directory):\n",
        "        folder_path = os.path.join(directory, subdir)\n",
        "        for fname in os.listdir(folder_path):\n",
        "            fpath = os.path.join(folder_path, fname)\n",
        "            try:\n",
        "                img = Image.open(fpath)\n",
        "                img.verify()  # Check for corruption\n",
        "            except (UnidentifiedImageError, OSError):\n",
        "                os.remove(fpath)\n",
        "                num_removed += 1\n",
        "    print(f\"Removed {num_removed} corrupted images.\")\n",
        "\n",
        "# Run it for each split\n",
        "remove_corrupted_images(train_dir)\n",
        "remove_corrupted_images(val_dir)\n",
        "remove_corrupted_images(test_dir)\n",
        "history = model.fit(train_data, validation_data=val_data, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-nG5iqE2MuK",
        "outputId": "36e38c19-2024-40d7-f4e0-49801ce4192c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed 0 corrupted images.\n",
            "Removed 0 corrupted images.\n",
            "Removed 0 corrupted images.\n",
            "Epoch 1/10\n",
            "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1324s\u001b[0m 5s/step - accuracy: 0.9808 - loss: 0.0684 - val_accuracy: 0.9505 - val_loss: 0.1954\n",
            "Epoch 2/10\n",
            "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1346s\u001b[0m 5s/step - accuracy: 0.9783 - loss: 0.0748 - val_accuracy: 0.9505 - val_loss: 0.2209\n",
            "Epoch 3/10\n",
            "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1316s\u001b[0m 5s/step - accuracy: 0.9820 - loss: 0.0753 - val_accuracy: 0.9650 - val_loss: 0.3687\n",
            "Epoch 4/10\n",
            "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1326s\u001b[0m 5s/step - accuracy: 0.9809 - loss: 0.0692 - val_accuracy: 0.9288 - val_loss: 1.2183\n",
            "Epoch 5/10\n",
            "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1344s\u001b[0m 5s/step - accuracy: 0.9849 - loss: 0.0872 - val_accuracy: 0.9409 - val_loss: 0.3076\n",
            "Epoch 6/10\n",
            "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1366s\u001b[0m 5s/step - accuracy: 0.9876 - loss: 0.0483 - val_accuracy: 0.9614 - val_loss: 0.1990\n",
            "Epoch 7/10\n",
            "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1321s\u001b[0m 5s/step - accuracy: 0.9867 - loss: 0.0569 - val_accuracy: 0.9650 - val_loss: 0.1405\n",
            "Epoch 8/10\n",
            "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1333s\u001b[0m 5s/step - accuracy: 0.9895 - loss: 0.0390 - val_accuracy: 0.9614 - val_loss: 0.6128\n",
            "Epoch 9/10\n",
            "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1326s\u001b[0m 5s/step - accuracy: 0.9866 - loss: 0.0484 - val_accuracy: 0.9626 - val_loss: 0.2030\n",
            "Epoch 10/10\n",
            "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1332s\u001b[0m 5s/step - accuracy: 0.9889 - loss: 0.0407 - val_accuracy: 0.9831 - val_loss: 0.0907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_data)"
      ],
      "metadata": {
        "id": "JHLS3IYd1KA-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b27ba9cb-94de-49e4-da63-2cede86014f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9896 - loss: 0.0443\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.02962850220501423, 0.9940711259841919]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 31148,
          "sourceId": 3362,
          "sourceType": "competition"
        }
      ],
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}